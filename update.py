import os
import requests
from bs4 import BeautifulSoup
import yfinance as yf
import datetime
import re

# === ç¢ºä¿ index.htm å­˜åœ¨ ===
html_path = "index.html"
if not os.path.exists(html_path):
    with open(html_path, "w", encoding="utf-8") as f:
        f.write("""<!DOCTYPE html>
<html lang="zh-TW">
<head>
<meta charset="UTF-8">
<title>Daily Report</title>
</head>
<body>
<h1>æ¯æ—¥åœ‹éš›ç¶“æ¿Ÿèˆ‡æ–°èå ±å‘Š</h1>

<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸</h2>
<ul></ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆè‹±æ–‡ï¼‰</h2>
<ul></ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆä¸­æ–‡ï¼‰</h2>
<ul></ul>

<h2>ğŸŒ æ”¿ç¶“å±€å‹¢æ‘˜è¦</h2>
<ul></ul>

</body>
</html>""")

# === å…¨çƒä¸»è¦è‚¡å¸‚å³æ™‚åƒ¹æ ¼ ===
markets = {
    "é“ç“ŠæŒ‡æ•¸ (DJI)": "^DJI",
    "NASDAQ": "^IXIC",
    "S&P 500": "^GSPC",
    "æ—¥ç¶“ 225": "^N225",
    "å°ç£åŠ æ¬Š": "^TWII",
    "ä¸Šè­‰æŒ‡æ•¸": "000001.SS",
    "å¾·åœ‹ DAX": "^GDAXI"
}

def fetch_markets():
    rows = ""
    for name, symbol in markets.items():
        try:
            ticker = yf.Ticker(symbol)
            price = round(ticker.fast_info["lastPrice"], 2)
            rows += f"<li>{name}: {price}</li>"
        except:
            rows += f"<li>{name}: è®€å–å¤±æ•—</li>"
    return rows

# === è‹±æ–‡æ–°è (Google News RSS) ===
def fetch_news_en():
    url = "https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en"
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        items = soup.find_all("item")[:20]
        news_list = []
        for item in items:
            title = item.title.text.strip()
            link = item.link.text.strip()
            news_list.append(f'<li><a href="{link}" target="_blank">{title}</a></li>')
        return "\n".join(news_list)
    except:
        return "<li>è‹±æ–‡æ–°èè®€å–å¤±æ•—</li>"

# === ä¸­æ–‡æ–°è (è¯åˆæ–°èç¶²åœ‹éš› RSS) ===
def fetch_news_zh():
    url = "https://udn.com/rssfeed/news/1/åœ‹éš›"
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "xml")
        items = soup.find_all("item")[:20]
        news_list = []
        for item in items:
            title = item.title.text.strip()
            link = item.link.text.strip()
            news_list.append(f'<li><a href="{link}" target="_blank">{title}</a></li>')
        return "\n".join(news_list)
    except:
        return "<li>ä¸­æ–‡æ–°èè®€å–å¤±æ•—</li>"

# === æ”¿ç¶“å±€å‹¢ (Reuters World) ===
def fetch_geo():
    url = "https://www.reuters.com/world/"
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        articles = soup.select("a[href*='/world/']")[:20]
        geo_html = ""
        for a in articles:
            title = a.get_text(strip=True)
            link = "https://www.reuters.com" + a.get("href")
            geo_html += f'<li><a href="{link}" target="_blank">{title}</a></li>'
        return geo_html
    except:
        return "<li>æ”¿ç¶“å±€å‹¢è®€å–å¤±æ•—</li>"

# === æ›´æ–° index.htm ===
def update_html():
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    with open(html_path, "r", encoding="utf-8") as f:
        html = f.read()

    # åˆªé™¤èˆŠè³‡æ–™å€å¡Š
    html = re.sub(r"<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸.*?<\/ul>", "", html, flags=re.S)
    html = re.sub(r"<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆè‹±æ–‡ï¼‰.*?<\/ul>", "", html, flags=re.S)
    html = re.sub(r"<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆä¸­æ–‡ï¼‰.*?<\/ul>", "", html, flags=re.S)
    html = re.sub(r"<h2>ğŸŒ æ”¿ç¶“å±€å‹¢æ‘˜è¦.*?<\/ul>", "", html, flags=re.S)

    # æ–°è³‡æ–™
    new_content = f"""
<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸ï¼ˆæ›´æ–°æ™‚é–“ï¼š{now}ï¼‰</h2>
<ul>
{fetch_markets()}
</ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆè‹±æ–‡ï¼‰</h2>
<ul>
{fetch_news_en()}
</ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆä¸­æ–‡ï¼‰</h2>
<ul>
{fetch_news_zh()}
</ul>

<h2>ğŸŒ æ”¿ç¶“å±€å‹¢æ‘˜è¦</h2>
<ul>
{fetch_geo()}
</ul>
</body>
"""
    html = html.replace("</body>", new_content)

    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html)

if __name__ == "__main__":
    update_html()
    print("é¦–é æ›´æ–°å®Œæˆ âœ…")
