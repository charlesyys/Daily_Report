import requests
from bs4 import BeautifulSoup
import yfinance as yf
import datetime

# === 1. å…¨çƒä¸»è¦è‚¡å¸‚å³æ™‚åƒ¹æ ¼ ===
markets = {
    "é“ç“ŠæŒ‡æ•¸ (DJI)": "^DJI",
    "NASDAQ": "^IXIC",
    "S&P 500": "^GSPC",
    "æ—¥ç¶“ 225": "^N225",
    "å°ç£åŠ æ¬Š": "^TWII",
    "ä¸Šè­‰æŒ‡æ•¸": "000001.SS",
    "å¾·åœ‹ DAX": "^GDAXI"
}

def fetch_markets():
    rows = ""
    for name, symbol in markets.items():
        ticker = yf.Ticker(symbol)
        try:
            price = ticker.fast_info["lastPrice"]
            price = round(price, 2)
            rows += f"<li>{name}: {price}</li>"
        except:
            rows += f"<li>{name}: è®€å–å¤±æ•—</li>"
    return rows

# === 2. åœ‹éš›é‡å¤§æ–°èï¼ˆGoogle News RSSï¼‰ ===
def fetch_news():
    url = "https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant"

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        ),
        "Accept": "text/xml,application/xml,application/xhtml+xml",
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        "Referer": "https://news.google.com/",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Connection": "keep-alive"
    }

    # â˜… ç¦æ­¢ redirectï¼ŒæŠŠ example.com æ“‹æ‰
    r = requests.get(url, headers=headers, timeout=10, allow_redirects=False)

    # â˜… å¦‚æœè¢«å·å·è½‰å€ï¼Œç›´æ¥éŒ¯èª¤æé†’
    if r.status_code in (301, 302, 303, 307, 308):
        raise Exception("Google RSS è¢« redirect â†’ å¯èƒ½è¢«é¢¨æ§ï¼Œéœ€è¦æ› IP æˆ– Proxy")

    text = r.text.strip()
    if "Example Domain" in text:
        raise Exception("âš ï¸ RSS è¢«åçˆ¬èŸ²å°å‘ example.comï¼éœ€è¦æ›´å¼· headers æˆ– proxy")

    soup = BeautifulSoup(text, "xml")

    items = soup.find_all("item")[:10]
    news_html = ""

    for item in items:
        title = item.title.text
        link = item.link.text
        news_html += f"<li><a href='{link}' target='_blank'>{title}</a></li>"

    return news_html

# === 3. æ”¿ç¶“å±€å‹¢ï¼ˆReuters Worldï¼‰ ===
def fetch_geo():
    url = "https://www.reuters.com/world/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
    }
    r = requests.get(url, headers=headers, timeout=10)
    soup = BeautifulSoup(r.text, "html.parser")
    articles = soup.select("a[href*='/world/']")[:8]
    geo_html = ""
    for a in articles:
        title = a.get_text(strip=True)
        link = "https://www.reuters.com" + a.get("href")
        geo_html += f"<li><a href='{link}' target='_blank'>{title}</a></li>"
    return geo_html

# === 4. æ›´æ–° index.html ===
def update_html():
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    with open("index.html", "r", encoding="utf-8") as f:
        html = f.read()

    # åˆªé™¤èˆŠè³‡æ–™å€å¡Š
    import re
    html = re.sub(r"<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸.*</body>", "</body>", html, flags=re.S)

    # æ’å…¥æ–°è³‡æ–™
    new_content = f"""
<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸ï¼ˆæ›´æ–°æ™‚é–“ï¼š{now}ï¼‰</h2>
<ul>
{fetch_markets()}
</ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°è</h2>
<ul>
{fetch_news()}
</ul>

<h2>ğŸŒ æ”¿ç¶“å±€å‹¢æ‘˜è¦</h2>
<ul>
{fetch_geo()}
</ul>
</body>
"""
    html = html.replace("</body>", new_content)

    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html)

if __name__ == "__main__":
    update_html()
    print("é¦–é æ›´æ–°å®Œæˆ âœ…")
