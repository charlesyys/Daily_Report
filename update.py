import requests
from bs4 import BeautifulSoup
import yfinance as yf
import datetime
import re

# === å…¨çƒä¸»è¦è‚¡å¸‚å³æ™‚åƒ¹æ ¼ ===
markets = {
    "é“ç“ŠæŒ‡æ•¸ (DJI)": "^DJI",
    "NASDAQ": "^IXIC",
    "S&P 500": "^GSPC",
    "æ—¥ç¶“ 225": "^N225",
    "å°ç£åŠ æ¬Š": "^TWII",
    "ä¸Šè­‰æŒ‡æ•¸": "000001.SS",
    "å¾·åœ‹ DAX": "^GDAXI"
}

def fetch_markets():
    rows = ""
    for name, symbol in markets.items():
        ticker = yf.Ticker(symbol)
        try:
            price = ticker.fast_info["lastPrice"]
            price = round(price, 2)
            rows += f"<li>{name}: {price}</li>"
        except:
            rows += f"<li>{name}: è®€å–å¤±æ•—</li>"
    return rows

# === è‹±æ–‡æ–°èï¼ˆGoogle News RSSï¼‰ ===
def fetch_news_en():
    url = "https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en"
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        items = soup.find_all("item")[:20]

        news_html = ""
        for item in items:
            title = item.title.text.strip()
            link = item.guid.text.strip() if item.guid else item.link.text.strip()
            news_html += f'<li><a href="{link}" target="_blank">{title}</a></li>'
        return news_html
    except Exception as e:
        return f"<li>è‹±æ–‡æ–°èè®€å–å¤±æ•—: {e}</li>"

# === ä¸­æ–‡æ–°èï¼ˆè¯åˆæ–°èç¶² + ä¸­å¤®ç¤¾ï¼‰ ===
def fetch_news_zh():
    sources = [
        ("è¯åˆæ–°èç¶²åœ‹éš›", "https://udn.com/rssfeed/news/1003/6638?ch=news"),
        ("ä¸­å¤®ç¤¾åœ‹éš›", "https://www.cna.com.tw/rss/firstnews_rss.xml")
    ]
    news_html = ""
    for source, url in sources:
        try:
            r = requests.get(url, timeout=10)
            soup = BeautifulSoup(r.text, "xml")
            items = soup.find_all("item")[:10]
            for item in items:
                title = item.title.text.strip()
                link = item.link.text.strip()
                news_html += f'<li><a href="{link}" target="_blank">{title}</a> <small>({source})</small></li>'
        except Exception as e:
            news_html += f"<li>{source} è®€å–å¤±æ•—: {e}</li>"
    return news_html

# === æ”¿ç¶“å±€å‹¢ï¼ˆReuters Worldï¼‰ ===
def fetch_geo():
    url = "https://www.reuters.com/world/"
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        articles = soup.select("a[href*='/world/']")[:8]
        geo_html = ""
        for a in articles:
            title = a.get_text(strip=True)
            link = "https://www.reuters.com" + a.get("href")
            geo_html += f'<li><a href="{link}" target="_blank">{title}</a></li>'
        return geo_html
    except Exception as e:
        return f"<li>æ”¿ç¶“å±€å‹¢è®€å–å¤±æ•—: {e}</li>"

# === æ›´æ–° index.html ===
def update_html():
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    html_path = "index.html"

    # è®€å–ç¾æœ‰ HTML
    try:
        with open(html_path, "r", encoding="utf-8") as f:
            html = f.read()
    except FileNotFoundError:
        html = "<html><head><meta charset='utf-8'><title>Daily Report</title></head><body></body></html>"

    # åˆªé™¤èˆŠè³‡æ–™å€å¡Š
    html = re.sub(r"<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸.*</body>", "</body>", html, flags=re.S)

    # æ–°è³‡æ–™å€å¡Š
    new_content = f"""
<h2>ğŸ“ˆ å…¨çƒè‚¡å¸‚æŒ‡æ•¸ï¼ˆæ›´æ–°æ™‚é–“ï¼š{now}ï¼‰</h2>
<ul>
{fetch_markets()}
</ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆè‹±æ–‡ï¼‰</h2>
<ul>
{fetch_news_en()}
</ul>

<h2>ğŸ“° åœ‹éš›é‡å¤§æ–°èï¼ˆä¸­æ–‡ï¼‰</h2>
<ul>
{fetch_news_zh()}
</ul>

<h2>ğŸŒ æ”¿ç¶“å±€å‹¢æ‘˜è¦</h2>
<ul>
{fetch_geo()}
</ul>
</body>
"""
    html = html.replace("</body>", new_content)

    # å¯«å› HTML
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html)

if __name__ == "__main__":
    update_html()
    print("é¦–é æ›´æ–°å®Œæˆ âœ…")
